{
    "Missions": {
        "taskstitle": "Few tasks made...",
        "toolstitle": "Most relevant technologies",
        "bouygues": {
            "title": "DevOps BigData ",
            "location": "Bouygues Telecom",
            "dateShort": "2019-2021",
            "description": "In this initial assignment, I joined a team of 3 DevOps and 3 developers to strengthen Bouygues, a french telecommunication company, requirement for implementing a new platform.\n\nThe business requirement was to collect, aggregate, and index telecommunication data (antenna logs, routers, etc.) and serve them in a Hadoop cluster for BI purposes. Ultimately, the client aimed to build a platform to develop artificial intelligence capable of detecting network issues, such as outages on the Bouygues network.",
            "tasks": "Kubernetes Integration: Due to security constraints, the client opted not to migrate to the cloud and insisted on keeping their infrastructure within French data centers. Therefore, the Kubernetes cluster setup was done bare metal on machines provided by the client.\nHortonWorks Integration: Implementation of a Hadoop BigData cluster (formerly HortonWorks) to serve as a technical foundation for providing data to the client's data scientists.\nInfrastructure Industrialization with Ansible: Development of recipes to industrialize the deployment of the entire infrastructure (Kubernetes cluster and Hadoop cluster).\nMonitoring and Alerting Implementation with Prometheus, Grafana, and Alertmanager: Collection of relevant metrics in Prometheus and creation of alerting rules. Creation of Grafana dashboards. Implementation of a Prometheus metrics lifecycle policy; the client wanted to retain a significant historical record of these data."
        },
        "altrnativ": {
            "title": "DevOps BigData ",
            "location": "Altrnativ",
            "dateShort": "2021-2021",
            "description": "In this assignment, the specifications were to design and implement a new infrastructure for collecting, aggregating, and indexing telecommunication data, serving it in a Hadoop cluster for Data Science purposes.\n\nThe client, having heard about the successful implementation within the Bouygues group, wished to replicate a similar infrastructure for similar needs.",
            "tasks": "Kubernetes Integration: By choice, the client preferred a local installation of their infrastructure rather than on a cloud provider. The implementation was therefore done on machines provided by the client.\nHortonWorks Integration: Deployment of a Hadoop BigData cluster (formerly HortonWorks) to serve as a technical foundation for providing data to the client's data scientists.\nInfrastructure Industrialization with Ansible: Development of recipes to industrialize the deployment of the entire infrastructure (Kubernetes cluster and Hadoop cluster).\nMonitoring and Alerting Implementation with Prometheus, Grafana, and Alertmanager: Collection of relevant metrics in Prometheus and creation of alerting rules. Creation of Grafana dashboards. Implementation of a Prometheus metrics lifecycle policy; the client wanted to retain a significant historical record of these data."
        },
        "cdiscount": {
            "title": "SRE/DevOps ",
            "location": "Cdiscount",
            "dateShort": "2021-2022",
            "description": "At Cdiscount, the leading French B2C e-commerce and logistics company, and a true competitor to Amazon in the French market, I was part of the SRE team (approximately 8 members). My role involved developing and managing a significant portion of the Cdiscount platform, hosted across multiple data centers.\n\nCdiscount takes a unique approach, opting to retain in-house management rather than utilizing a cloud provider in Platform as a Service mode. This insightful mission allowed me to enhance my skills in infrastructure as code using Ansible and Terraform.\n\nAnother aspect of this mission was to modernize the distributed storage solution for Kubernetes by implementing Rook-Ceph.",
            "tasks": "Development and management of Kubernetes clusters: Writing Ansible playbooks and Terraform plans to administer and industrialize the infrastructure.\nIntegration of the replicated storage solution rook-ceph for stateful workloads in Kubernetes: A dedicated Ceph cluster was already in place but was too cumbersome to manage, so the decision was made to transition this technology into Kubernetes to simplify its management.\nCI/CD pipeline development: for development teams and SRE."
        },
        "mirakl": {
            "title": "SRE/DevOps ",
            "location": "Mirakl",
            "dateShort": "2022-now",
            "description": "At Mirakl, a true French unicorn and a leader in B2B e-commerce, the workforce has seen substantial growth and expansion in recent years.\n\nFor my first freelance mission, I joined the 15-member SRE team to assist in managing the multi-cloud infrastructure (GCP, AWS & Azure) to meet the increasing demands. Despite being closely aligned with DevOps culture, Mirakl faced technical debt due to its exponential growth.\n\nOne of the primary focuses of this mission is to modernize the entire infrastructure-as-code, incorporating Terraform, CD pipelines with ArgoCD, and updating the database infrastructure by integrating it into Kubernetes through Cloudnative PG.",
            "tasks": "Refactoring of the infrastructure as code: The client had accumulated technical debt in the Terraform infrastructure as code (IaC) section. All modules and resources were housed in a monorepo. The objective was therefore to refactor and restructure the Terraform modules, making them live and released in dedicated repositories to simplify management.\nDevelopment of new Kubernetes clusters: Overhauling existing clusters using the new Terraform stack on AWS and GCP, with new openings on Azure.\nRefactoring of the Git-Ops section: Similar to the IaC stack, the Git-Ops section resided in a monorepo. Again, the objective was to refactor the entire section to split the application packaging pushed to Kubernetes into dedicated repositories. The chosen solution: creating dedicated Helm charts for each application and orchestrating everything via ArgoCD.\nIntegration of a bastion within the infrastructure: We chose Teleport to meet the need for access to Kubernetes, DB, Kafka, and other applications proxied by Teleport.\nMigration of DB into Kubernetes with Cloudnative PG: The database aspect is one of Mirakl's heavy subjects. It was only natural to find a way to lighten this part. The Cloudnative PG project seemed promising to us, allowing us to house our databases inside Kubernetes and thus take advantage of its flexibility, stability, and resilience."
        }
    }
}
